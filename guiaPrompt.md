-----

## üõ†Ô∏è Gu√≠a R√°pida: Ingenier√≠a de Prompts para Especialistas en RRHH

El **Prompt Engineering** es la clave para que la inteligencia artificial se comporte exactamente como esperamos. Nuestro objetivo es modificar el **Mensaje del Sistema (`SystemMessage`)** para que el chatbot se convierta en un asistente de RRHH riguroso, √∫til y confiable, maximizando la utilidad del **RAG**.

### üìç Ubicaci√≥n del C√≥digo a Modificar

El c√≥digo que debe ser ajustado se encuentra dentro de la funci√≥n `responder_pregunta` en el archivo **`app.py`**.

### üß† Aplicando las 4 Mejores Pr√°cticas de Prompt Engineering

Ajustaremos el `SystemMessage` en cuatro √°reas clave: Rol, Contexto RAG, Restricciones y Tono/Formato.

#### 1\. üßë‚Äçüíª Asignaci√≥n de Rol Precisa (El "Qui√©n Eres")

Dale al LLM una **autoridad espec√≠fica** para que filtre mejor sus respuestas.

  * **Pr√°ctica:** Asigna un rol formal y especializado.
  * **Ejemplo:** Cambiar de "asistente amigable" a **"Consultor Experto en Pol√≠ticas de RRHH"**.

#### 2\. üìñ Directiva de Contexto RAG (La Clave del Experto)

Esta es la pr√°ctica m√°s importante para el RAG. Obliga al modelo a usar la informaci√≥n de los documentos (`context`) y evita que **alucine** (invente hechos).

  * **Pr√°ctica:** Usa frases imperativas para el *grounding*.
  * **Ejemplo:** "**Utiliza exclusivamente el contexto de documentos proporcionado (RAG)** para formular tu respuesta. Nunca inventes o extrapoles informaci√≥n."

#### 3\. üöß Guardarra√≠les y Restricciones (El "Qu√© NO Hacer")

Establece l√≠mites claros para evitar que el chatbot responda a temas fuera de su *expertise* (ej. legales o financieros complejos).

  * **Pr√°ctica:** Define una **instrucci√≥n de fallo seguro**.
  * **Ejemplo:** "Si la pregunta no se puede responder con el contexto o si es sobre temas legales/financieros, responde: 'Lamento no poder ofrecerte una respuesta espec√≠fica sobre ese tema. Por favor, consulta al equipo legal o al manual completo.'"

#### 4\. üìù Especificaci√≥n de Tono y Formato (El "C√≥mo Responder")

Asegura una experiencia de usuario clara y profesional.

  * **Pr√°ctica:** Indica el tono y el uso de elementos de formato.
  * **Ejemplo:** "Responde en un tono **profesional pero emp√°tico**. Usa **listas o negritas** para hacer la informaci√≥n f√°cil de digerir."

-----

## üíª C√≥digo Final `app.py` (Con Prompt Engineering)

Este es el c√≥digo final completo del archivo `app.py` con las mejores pr√°cticas de Prompt Engineering aplicadas en el `SystemMessage` y con la l√≥gica de RAG ya integrada.

```python
import os  
import streamlit as st
from langchain_ollama import OllamaEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# --- CONFIGURACI√ìN DE MODELOS Y RUTA DE LA BD VECTORIAL ---
# LLM que usar√° Ollama para generar las respuestas
LLM_MODEL = "deepseek-r1:1.5b"
# Modelo que usar√° Ollama para crear los embeddings
EMBEDDING_MODEL = "qwen2.5:0.5b"
# Carpeta donde se guard√≥ el √≠ndice FAISS generado en knowledge_base.py
VECTOR_STORE_PATH = "faiss_index_rh"

# üé® Configuraci√≥n b√°sica de la p√°gina de Streamlit
st.set_page_config(
    page_title="Chatbot RH Experto (RAG)",
    page_icon="üß†",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# üßº CSS: modo oscuro b√°sico para la app (fondo negro, texto blanco)
st.markdown(
    """
    <style>
    .stApp {background-color: #000000 !important; color: #ffffff !important;}
    * {color: #ffffff !important;}
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .block-container {max-width: 900px; padding-top: 2rem; padding-bottom: 2rem;}
    h1, h2, h3, h4, h5, h6 {text-align: center; color: #ffffff !important;}
    section[data-testid="stSidebar"] {background-color: #111111 !important;}
    .stCode, code {color: #ffffff !important; background-color: #222222 !important;}
    .stChatMessage {background-color: #111111 !important; color: #ffffff !important;}
    </style>
    """,
    unsafe_allow_html=True
)

# üß± Encabezado principal
st.title("üß† Chatbot RH Experto (RAG)")
st.caption("Asistente que combina IA local con tus documentos de RRHH para responder sobre pol√≠ticas internas.")

# üéØ Sidebar con explicaci√≥n de la app y ejemplos de uso
with st.sidebar:
    st.subheader("Acerca de este chatbot RAG")
    st.write(
        f"""
        Este asistente usa el modelo de lenguaje local `{LLM_MODEL}` y un √≠ndice vectorial
        creado a partir de tus documentos internos de Recursos Humanos.
        
        Cuando haces una pregunta, primero busca fragmentos relevantes en esos documentos
        y luego genera una respuesta basada en ese contexto.
        """
    )
    st.markdown("---")
    st.write("üí° Tip: Pregunta por tus documentos cargados en la carpeta `docs`.")
    st.code("¬øCu√°l es el proceso para solicitar tiempo libre seg√∫n el PDF de pol√≠ticas?")

# --- FUNCIONES CLAVE DE LA IA ---

# 1. Funci√≥n para cargar el modelo de Ollama (LLM y Embeddings)
@st.cache_resource
def cargar_recursos():
    # üß† Carga del LLM (Generador de Respuesta)
    llm = ChatOllama(
        model=LLM_MODEL, 
        temperature=0.2 
    )
    
    # üìö Carga de Embeddings (Buscador de Vectores)
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
    
    # üóÉÔ∏è Carga del Almac√©n Vectorial (FAISS Index)
    if os.path.exists(VECTOR_STORE_PATH):
        vector_store = FAISS.load_local(
            VECTOR_STORE_PATH, 
            embeddings, 
            allow_dangerous_deserialization=True # Necesario para cargar FAISS de disco
        )
        # El retriever es el componente que busca los documentos m√°s relevantes
        retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # Busca los 3 fragmentos m√°s relevantes
        return llm, retriever
    else:
        st.error(f"¬°Error! No se encontr√≥ la base de conocimiento en '{VECTOR_STORE_PATH}'. Ejecuta 'python knowledge_base.py' primero.")
        return llm, None

# 2. Funci√≥n que env√≠a la pregunta al modelo incluyendo el RAG
def responder_pregunta_rag(pregunta: str, llm, retriever) -> str:
    if not retriever:
        return "El sistema RAG no est√° inicializado. Por favor, verifica la base de conocimiento y reinicia."

    # --- PLANTILLA DEL PROMPT OPTIMIZADA CON MEJORES PR√ÅCTICAS DE PE ---
    
    # Esta plantilla define el rol, las reglas RAG y el formato.
    prompt_template = ChatPromptTemplate.from_messages(
        [
            SystemMessage(content="""
                Eres un **Consultor Experto en Pol√≠ticas de RRHH** y Beneficios.

                **Instrucci√≥n Clave (RAG):**
                1. Utiliza **exclusivamente el contexto de documentos proporcionado** (RAG) en la secci√≥n 'context' para formular tu respuesta. Nunca inventes o extrapoles informaci√≥n.
                2. Mant√©n la coherencia con el historial de conversaci√≥n, pero prioriza la informaci√≥n del contexto RAG.

                **Reglas de Respuesta:**
                * Responde en espa√±ol, con un tono profesional pero emp√°tico.
                * Siempre utiliza **listas, vi√±etas o negritas** para hacer la informaci√≥n concisa y f√°cil de leer.

                **Guardarra√≠l:**
                * Si la pregunta no se puede responder con el contexto de documentos o es sobre temas legales/financieros, responde: "Lamento no poder ofrecerte una respuesta espec√≠fica sobre ese tema. Por favor, consulta al equipo legal o al manual completo."

                **Contexto de Documentos (RAG):**
                {context}
                """),
            *convertir_historial_a_mensajes(st.session_state.messages),
            HumanMessage(content="{input}"),
        ]
    )

    # 3. Creaci√≥n de la cadena de documentos (combina la pregunta con los documentos recuperados)
    document_chain = create_stuff_documents_chain(llm, prompt_template)
    
    # 4. Creaci√≥n de la cadena de recuperaci√≥n (combina el retriever con la cadena de documentos)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    
    # 5. Llamamos al cerebro RAG
    # El historial se pasa impl√≠citamente en el prompt_template
    respuesta = retrieval_chain.invoke({"input": pregunta})
    
    return respuesta["answer"]

# Funci√≥n auxiliar para convertir el historial de Streamlit a un formato que LangChain entienda
def convertir_historial_a_mensajes(mensajes_historial):
    mensajes_langchain = []
    # Ignoramos el primer mensaje (el mensaje de bienvenida)
    for msg in mensajes_historial[1:]: 
        if msg["role"] == "user":
            mensajes_langchain.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            mensajes_langchain.append(AIMessage(content=msg["content"]))
    return mensajes_langchain

# üß† Estado de la conversaci√≥n (Mensaje de bienvenida)
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": (
                "üëã ¬°Hola! Soy tu Chatbot Experto de Recursos Humanos.\n\n"
                "Estoy listo para responder preguntas sobre nuestras pol√≠ticas internas cargadas. Preg√∫ntame con confianza."
            ),
        }
    ]

# Intentar cargar recursos al inicio
llm, retriever = cargar_recursos()

# üí¨ Mostrar historial del chat
for msg in st.session_state.messages:
    avatar = "üß†" if msg["role"] == "assistant" else "üßë‚Äçüíº"
    with st.chat_message("assistant" if msg["role"] == "assistant" else "user", avatar=avatar):
        st.markdown(msg["content"])

# üßæ Input del usuario
prompt = st.chat_input("Escribe tu pregunta sobre pol√≠ticas internas aqu√≠...")

if prompt:
    # 1. A√±adir pregunta del usuario al historial
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="üßë‚Äçüíº"):
        st.markdown(prompt)

    # 2. Generar respuesta con RAG
    with st.chat_message("assistant", avatar="üß†"):
        with st.spinner("Buscando en los documentos y pensando la mejor respuesta para ti... üí≠"):
            if retriever:
                # ¬°Aqu√≠ llamamos a la IA real con RAG!
                respuesta = responder_pregunta_rag(prompt, llm, retriever)
            else:
                respuesta = "El sistema RAG no pudo inicializarse correctamente. Por favor, revisa la consola para ver los errores."
            
            st.markdown(respuesta)
            
    # 3. A√±adir respuesta de la IA al historial
    st.session_state.messages.append({"role": "assistant", "content": respuesta})

```
